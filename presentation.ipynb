{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personality prediction from tweet\n",
    "\n",
    "by Angelo Basile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(113) #set seed before any keras import\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from keras.preprocessing import sequence\n",
    "from collections import Counter\n",
    "import pydot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>mbti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>206814204</td>\n",
       "      <td>it</td>\n",
       "      <td>['prostituzione intellettuale (cit.futura) htt...</td>\n",
       "      <td>ENFJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>1520373739</td>\n",
       "      <td>it</td>\n",
       "      <td>['@nanawani_ haha grazie a Dio la lau ci fa la...</td>\n",
       "      <td>INTJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>883135740</td>\n",
       "      <td>it</td>\n",
       "      <td>[\"Certo, poi devo prendere ripetizioni di anal...</td>\n",
       "      <td>INFJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>13264792</td>\n",
       "      <td>de</td>\n",
       "      <td>['@GoldeCarlsson @theRosenblatts Ich denk die ...</td>\n",
       "      <td>INFP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>169452362</td>\n",
       "      <td>it</td>\n",
       "      <td>['Un ora e mezza di attesa. Bello schifo', 'Da...</td>\n",
       "      <td>INFP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id lang                                               text  mbti\n",
       "436   206814204   it  ['prostituzione intellettuale (cit.futura) htt...  ENFJ\n",
       "121  1520373739   it  ['@nanawani_ haha grazie a Dio la lau ci fa la...  INTJ\n",
       "214   883135740   it  [\"Certo, poi devo prendere ripetizioni di anal...  INFJ\n",
       "885    13264792   de  ['@GoldeCarlsson @theRosenblatts Ich denk die ...  INFP\n",
       "104   169452362   it  ['Un ora e mezza di attesa. Bello schifo', 'Da...  INFP"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed=0\n",
    "corpus = pd.read_csv('twistytest.csv', \n",
    "                     index_col=0, \n",
    "                     header=1, \n",
    "                     names=['user_id', 'lang', 'text', 'mbti'])\n",
    "corpus.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136, 4)\n"
     ]
    }
   ],
   "source": [
    "#here we limit the corpus size. The SVM with all the text can learn somethign\n",
    "corpus.text = corpus.text.apply(lambda x: x[:1000]) \n",
    "corpus.mbti = corpus.mbti.apply(lambda x: x[0])\n",
    "\n",
    "#corpus = tmp.sample(frac=1, random_state=seed)\n",
    "e = corpus[corpus.mbti.apply(lambda x: x == 'E')]\n",
    "i = corpus[corpus.mbti.apply(lambda x: x == 'I')].sample(226)\n",
    "corpus = pd.concat([e,i]).sample(frac=0.3, random_state=seed)\n",
    "print(corpus.shape)\n",
    "\n",
    "## set max length of doc per author\n",
    "sentences = corpus.text#.apply(lambda x: x[:100000])\n",
    "## trim labels: convert problem to binary classification I vs E\n",
    "labels = corpus.mbti\n",
    "\n",
    "## make sure we have a label for every data instance\n",
    "assert(len(sentences)==len(labels))\n",
    "data={}\n",
    "np.random.seed(113) #seed\n",
    "data['target']= np.random.permutation(labels)\n",
    "np.random.seed(113) # use same seed!\n",
    "data['data'] = np.random.permutation(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(136, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "      <th>mbti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>430124293</td>\n",
       "      <td>de</td>\n",
       "      <td>['@Schmidtlepp Frage vor Wahl zum BuPräs:gesch...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>621638341</td>\n",
       "      <td>de</td>\n",
       "      <td>[\"damn. i'm here again.\", \"@chaospiral hey, i'...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>181297248</td>\n",
       "      <td>de</td>\n",
       "      <td>['Der @eigensinn83 schwafelt meine Timeline zu...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>271514739</td>\n",
       "      <td>it</td>\n",
       "      <td>['\"Tu con le fave dovevi intendertene! E poi i...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>1083774439</td>\n",
       "      <td>de</td>\n",
       "      <td>['Trailer Salzburger Festspiele 2013: http://t...</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id lang                                               text mbti\n",
       "610   430124293   de  ['@Schmidtlepp Frage vor Wahl zum BuPräs:gesch...    E\n",
       "836   621638341   de  [\"damn. i'm here again.\", \"@chaospiral hey, i'...    E\n",
       "527   181297248   de  ['Der @eigensinn83 schwafelt meine Timeline zu...    I\n",
       "170   271514739   it  ['\"Tu con le fave dovevi intendertene! E poi i...    I\n",
       "855  1083774439   de  ['Trailer Salzburger Festspiele 2013: http://t...    I"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview the dataset\n",
    "print(corpus.shape)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJDXO0EtS4wy9JDXufwFN58wGiiIuAAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcb61390be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution of labels\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "l, v = zip(*Counter(y_train).items())\n",
    "indexes = np.arange(len(l))\n",
    "width = 1\n",
    "plt.bar(indexes, v, width, color=['r', 'b'])\n",
    "plt.xticks(indexes + width * 0.5, l)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train instances: 86 #dev: 22 #test: 28\n"
     ]
    }
   ],
   "source": [
    "#split the data into train, dev, test\n",
    "\n",
    "X_rest, X_test, y_rest, y_test = train_test_split(data['data'], data['target'], test_size=0.2)\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_rest, y_rest, test_size=0.2)\n",
    "del X_rest, y_rest\n",
    "print(\"#train instances: {} #dev: {} #test: {}\".format(len(X_train),len(X_dev),len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "For the baseline we use an SVM with a sparse feature representation.\n",
    "\n",
    "We use both character- and word-ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pipeline = Pipeline([('features', FeatureUnion([('wngram', TfidfVectorizer(ngram_range=(1,2))),\n",
    "                                                ('cngram', TfidfVectorizer(analyzer='char'))])),\n",
    "                     ('cls', LinearSVC())])\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The SVM works quite well already: we outperform the random baseline by a signficant margin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.725274725275\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          E       0.65      0.74      0.69        38\n",
      "          I       0.79      0.72      0.75        53\n",
      "\n",
      "avg / total       0.73      0.73      0.73        91\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testpred = pipeline.predict(X_test)\n",
    "print(accuracy_score(testpred, y_test))\n",
    "print(classification_report(testpred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network\n",
    "\n",
    "First we have to encode the labels in the one-hot format. Since this is a binary classification format, we don't convert them to a categorical format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "y2i = defaultdict(lambda: len(y2i))\n",
    "y_train_num = [y2i[mbti] for mbti in y_train]\n",
    "y_dev_num = [y2i[mbti] for mbti in y_dev]\n",
    "y_test_num = [y2i[mbti] for mbti in y_test]\n",
    "num_classes = len(np.unique(y_train_num))\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text representation\n",
    "\n",
    "For the baseline we used a one-hot encoding. For our neural model we are going to represent the text using a dense representation. We will be building it from characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# convert words to indices, taking care of UNKs\n",
    "def get_characters(sentence, c2i):\n",
    "    out = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        chars = []\n",
    "        for c in word:\n",
    "            chars.append(c2i[c])\n",
    "        out.append(chars)\n",
    "    return out\n",
    "\n",
    "c2i = defaultdict(lambda: len(c2i))\n",
    "\n",
    "PAD = c2i[\"<pad>\"] # index 0 is padding\n",
    "UNK = c2i[\"<unk>\"] # index 1 is for UNK\n",
    "X_train_num = [get_characters(sentence, c2i) for sentence in X_train]\n",
    "c2i = defaultdict(lambda: UNK, c2i) # freeze - cute trick!\n",
    "X_dev_num = [get_characters(sentence, c2i) for sentence in X_dev]\n",
    "X_test_num = [get_characters(sentence, c2i) for sentence in X_test]\n",
    "\n",
    "max_sentence_length=max([len(s.split(\" \")) for s in X_train] \n",
    "                        + [len(s.split(\" \")) for s in X_dev] \n",
    "                        + [len(s.split(\" \")) for s in X_test] )\n",
    "max_word_length = max([len(word)  for sentence in X_train_num for word in sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n",
      "92\n",
      "[ '[\\'Herzlich Willkommen, kleiner Mats! &lt;3\\', \\'Nein. Selbst wenn man krank ist sollte man nicht ,,Familien im Brennpunkt\" schauen. Neinneinnein.\\', \\'Irgendwer hat heute was dagegen, dass ich zur Uni komme. Meine Schusseligkeit kennt keine Grenzen.\\', \\'Geschichten aus dem Waschsalon Kapitel I: ,,als mich ein alleinerziehender 40-Jähriger fragte, ob ich mal einen Kaffee mit ihm trinken gehe\"\\', \\'He, #SiZolli! Morgen hättest Du 90 Minuten durchspielen können, weißte selbst, ne?! #fck #effzeh #traditionsverein @Rote_Teufel_RT @fckoeln\\', \\'@FabianM_Mueller diese Geschichte wird mit Sicherheit kein zweites Kapitel bekommen ;)\\', \\'Es gibt da diese eine Person, der ich dafür danken möchte, mir die Musik von #Olson gezeigt zu haben. #thisgoesoutto #dreiTageGlück\\', \\',,generiert\" ist übrigens ein scheiß Wort und sollte nicht im Zusammenhang mit meinen Artikeln verwendet werden.\\', \\'@gelsen Rheinland-Pfälzischer Rosé hilft. Gegen alles.\\', \\'@gelsen hatte ich die letzten drei Tage auch. Es wird besser.\\', \\''\n",
      " '[\\'\"@rihanna: Mueller is always in the right position!\" Zitta e a 90!\\', \\'@BrindaThakore Ma va a cagare!\\', \"@BrindaThakore Yes, but I won\\'t\", \\'@BrindaThakore lol\\', \\'@BrindaThakore Hahaha, no.\\', \\'@brn_killers Ma tu hai capito che botta che ha preso? Quello è già fortunato se si regge ancora in piedi.\\', \\'\"@FINDINGZAY: \"@smolecolatevi: Vi comunico con enorme dispiacere che mio padre tifa l\\\\\\'Argentina\" lui sì che ragiona\" ecco!\\', \\'@g_beyle non sei all\\\\\\'altezza di quel \"burzum\".\\', \\'\"@xlucyshug: \"Grazie alla Disney\" pt. 20\\\\n\\\\nCrediamo nelle fate. http://t.co/J0rIE6DyQc\" @FedericaPirollo, ce ne sono molti altri\\', \\'Minchia, vogliamo parlare della ragazza in costume tra i tifosi tedeschi?!\\', \\'@animevuote ma chi vogliono darla a bere?\\', \\'@Wolfayn Non se tu avessi imparato i congiuntivi.\\', \\'@Wolfayn «Cristo» va in maiuscolo. Dopo «vuoi» ci vuole una virgola. Si dice «stammi bene» semmai.\\', \\'Germany is a 4 STAR #WorldChampion!\\\\n\\\\n#GERvsARG #GERvARG #GERARG #GER #ARG #GermaniaArgentina #Mondiali2014 ']\n",
      "[[2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [12, 9, 8, 8, 13, 14, 15, 15, 5, 16, 17], [13, 8, 5, 9, 16, 5, 6], [18, 19, 20, 21, 22], [23, 8, 20, 24, 25, 3, 17], [3, 26, 5, 9, 16, 27], [28, 5, 8, 29, 21, 20], [30, 5, 16, 16], [15, 19, 16], [13, 6, 19, 16, 13], [9, 21, 20], [21, 14, 8, 8, 20, 5], [15, 19, 16], [16, 9, 10, 11, 20], [17, 17, 31, 19, 15, 9, 8, 9, 5, 16], [9, 15], [32, 6, 5, 16, 16, 33, 34, 16, 13, 20, 35], [21, 10, 11, 19, 34, 5, 16, 27], [26, 5, 9, 16, 16, 5, 9, 16, 16, 5, 9, 16, 27, 3, 17], [3, 36, 6, 37, 5, 16, 38, 30, 5, 6], [11, 19, 20], [11, 5, 34, 20, 5], [30, 19, 21], [38, 19, 37, 5, 37, 5, 16, 17], [38, 19, 21, 21], [9, 10, 11], [7, 34, 6], [39, 16, 9], [13, 14, 15, 15, 5, 27], [18, 5, 9, 16, 5], [28, 10, 11, 34, 21, 21, 5, 8, 9, 37, 13, 5, 9, 20], [13, 5, 16, 16, 20], [13, 5, 9, 16, 5], [40, 6, 5, 16, 7, 5, 16, 27, 3, 17], [3, 40, 5, 21, 10, 11, 9, 10, 11, 20, 5, 16], [19, 34, 21], [38, 5, 15], [12, 19, 21, 10, 11, 21, 19, 8, 14, 16], [41, 19, 33, 9, 20, 5, 8], [36, 42], [17, 17, 19, 8, 21], [15, 9, 10, 11], [5, 9, 16], [19, 8, 8, 5, 9, 16, 5, 6, 7, 9, 5, 11, 5, 16, 38, 5, 6], [43, 44, 45, 46, 47, 11, 6, 9, 37, 5, 6], [48, 6, 19, 37, 20, 5, 17], [14, 29], [9, 10, 11], [15, 19, 8], [5, 9, 16, 5, 16], [41, 19, 48, 48, 5, 5], [15, 9, 20], [9, 11, 15], [20, 6, 9, 16, 13, 5, 16], [37, 5, 11, 5, 35, 3, 17], [3, 4, 5, 17], [49, 28, 9, 50, 14, 8, 8, 9, 22], [18, 14, 6, 37, 5, 16], [11, 47, 20, 20, 5, 21, 20], [51, 34], [52, 44], [18, 9, 16, 34, 20, 5, 16], [38, 34, 6, 10, 11, 21, 33, 9, 5, 8, 5, 16], [13, 53, 16, 16, 5, 16, 17], [30, 5, 9, 54, 20, 5], [21, 5, 8, 29, 21, 20, 17], [16, 5, 55, 22], [49, 48, 10, 13], [49, 5, 48, 48, 7, 5, 11], [49, 20, 6, 19, 38, 9, 20, 9, 14, 16, 21, 56, 5, 6, 5, 9, 16], [57, 58, 14, 20, 5, 59, 60, 5, 34, 48, 5, 8, 59, 58, 60], [57, 48, 10, 13, 14, 5, 8, 16, 3, 17], [3, 57, 31, 19, 29, 9, 19, 16, 18, 59, 18, 34, 5, 8, 8, 5, 6], [38, 9, 5, 21, 5], [40, 5, 21, 10, 11, 9, 10, 11, 20, 5], [30, 9, 6, 38], [15, 9, 20], [28, 9, 10, 11, 5, 6, 11, 5, 9, 20], [13, 5, 9, 16], [7, 30, 5, 9, 20, 5, 21], [41, 19, 33, 9, 20, 5, 8], [29, 5, 13, 14, 15, 15, 5, 16], [24, 61, 3, 17], [3, 62, 21], [37, 9, 29, 20], [38, 19], [38, 9, 5, 21, 5], [5, 9, 16, 5], [63, 5, 6, 21, 14, 16, 17], [38, 5, 6], [9, 10, 11], [38, 19, 48, 64, 6], [38, 19, 16, 13, 5, 16], [15, 53, 10, 11, 20, 5, 17], [15, 9, 6], [38, 9, 5], [18, 34, 21, 9, 13], [56, 14, 16], [49, 65, 8, 21, 14, 16], [37, 5, 7, 5, 9, 37, 20]]\n"
     ]
    }
   ],
   "source": [
    "### we need both max sent and word length\n",
    "print(max_sentence_length)\n",
    "print(max_word_length)\n",
    "print(X_train[0:2])\n",
    "print(X_train_num[0][:100]) # example how the first two sentences are encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_words(tensor_words, max_word_len, pad_symbol_id, max_sent_len=None):\n",
    "    \"\"\"\n",
    "    pad character list all to same word length\n",
    "    \"\"\"\n",
    "    padded = []\n",
    "    for words in tensor_words:\n",
    "        if max_sent_len: #pad all to same sentence length (insert empty word list)\n",
    "            words = [[[0]]*(max_sent_len-len(words))+ words][0] #prepending empty words\n",
    "        padded.append(sequence.pad_sequences(words, maxlen=max_word_len, value=pad_symbol_id))\n",
    "    return np.array(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_pad_char = pad_words(X_train_num, max_word_length, 0, max_sent_len=max_sentence_length)\n",
    "X_dev_pad_char = pad_words(X_dev_num, max_word_length, 0, max_sent_len=max_sentence_length)\n",
    "X_test_pad_char = pad_words(X_test_num, max_word_length, 0, max_sent_len=max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(86, 182, 92)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pad_char.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, GRU, TimeDistributed, Embedding, Bidirectional\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My model: WENP (WE Need more Power)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using a separate word embedding matrix, compose words through characters (see https://aclweb.org/anthology/W/W16/W16-4303.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=8\n",
    "max_chars = len(c2i)\n",
    "c_dim=50\n",
    "c_h_dim=32\n",
    "w_h_dim=32\n",
    "char_vocab_size = len(c2i)\n",
    "\n",
    "## lower-level character LSTM\n",
    "input_chars = Input(shape=(max_sentence_length, max_word_length), name='main_input')\n",
    "\n",
    "embedded_chars = TimeDistributed(Embedding(char_vocab_size, c_dim,\n",
    "                                         input_length=max_word_length), name='char_embedding')(input_chars)\n",
    "char_lstm = TimeDistributed(Bidirectional(GRU(c_h_dim)), name='GRU_on_char')(embedded_chars)\n",
    "\n",
    "word_lstm_from_char = Bidirectional(GRU(w_h_dim), name='GRU_on_words')(char_lstm)\n",
    "\n",
    "# And add a prediction node on top\n",
    "predictions = Dense(1, activation='relu', name='output_layer')(word_lstm_from_char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 182, 92)           0         \n",
      "_________________________________________________________________\n",
      "char_embedding (TimeDistribu (None, 182, 92, 50)       20500     \n",
      "_________________________________________________________________\n",
      "GRU_on_char (TimeDistributed (None, 182, 64)           15936     \n",
      "_________________________________________________________________\n",
      "GRU_on_words (Bidirectional) (None, 64)                18624     \n",
      "_________________________________________________________________\n",
      "output_layer (Dense)         (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 55,125\n",
      "Trainable params: 55,125\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=input_chars, outputs=predictions)\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"337pt\" viewBox=\"0.00 0.00 470.00 337.00\" width=\"470pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 333)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-333 466,-333 466,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140511421196216 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140511421196216</title>\n",
       "<polygon fill=\"none\" points=\"138,-292.5 138,-328.5 324,-328.5 324,-292.5 138,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231\" y=\"-306.8\">main_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140511421432944 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140511421432944</title>\n",
       "<polygon fill=\"none\" points=\".5,-219.5 .5,-255.5 461.5,-255.5 461.5,-219.5 .5,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231\" y=\"-233.8\">char_embedding(embedding_1): TimeDistributed(Embedding)</text>\n",
       "</g>\n",
       "<!-- 140511421196216&#45;&gt;140511421432944 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140511421196216-&gt;140511421432944</title>\n",
       "<path d=\"M231,-292.4551C231,-284.3828 231,-274.6764 231,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"234.5001,-265.5903 231,-255.5904 227.5001,-265.5904 234.5001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140511573940320 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140511573940320</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 462,-182.5 462,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231\" y=\"-160.8\">GRU_on_char(bidirectional_1): TimeDistributed(Bidirectional)</text>\n",
       "</g>\n",
       "<!-- 140511421432944&#45;&gt;140511573940320 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140511421432944-&gt;140511573940320</title>\n",
       "<path d=\"M231,-219.4551C231,-211.3828 231,-201.6764 231,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"234.5001,-192.5903 231,-182.5904 227.5001,-192.5904 234.5001,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140511518174456 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140511518174456</title>\n",
       "<polygon fill=\"none\" points=\"69.5,-73.5 69.5,-109.5 392.5,-109.5 392.5,-73.5 69.5,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231\" y=\"-87.8\">GRU_on_words(gru_2): Bidirectional(GRU)</text>\n",
       "</g>\n",
       "<!-- 140511573940320&#45;&gt;140511518174456 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140511573940320-&gt;140511518174456</title>\n",
       "<path d=\"M231,-146.4551C231,-138.3828 231,-128.6764 231,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"234.5001,-119.5903 231,-109.5904 227.5001,-119.5904 234.5001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140511518146456 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140511518146456</title>\n",
       "<polygon fill=\"none\" points=\"150.5,-.5 150.5,-36.5 311.5,-36.5 311.5,-.5 150.5,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"231\" y=\"-14.8\">output_layer: Dense</text>\n",
       "</g>\n",
       "<!-- 140511518174456&#45;&gt;140511518146456 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140511518174456-&gt;140511518146456</title>\n",
       "<path d=\"M231,-73.4551C231,-65.3828 231,-55.6764 231,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"234.5001,-46.5903 231,-36.5904 227.5001,-46.5904 234.5001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "86/86 [==============================] - 29s - loss: 0.8218 - acc: 0.5000    \n",
      "Epoch 2/10\n",
      "86/86 [==============================] - 31s - loss: 0.7154 - acc: 0.4651    \n",
      "Epoch 3/10\n",
      "86/86 [==============================] - 32s - loss: 0.6856 - acc: 0.5465    \n",
      "Epoch 4/10\n",
      "86/86 [==============================] - 31s - loss: 0.6935 - acc: 0.5465    \n",
      "Epoch 5/10\n",
      "86/86 [==============================] - 31s - loss: 0.6806 - acc: 0.5349    \n",
      "Epoch 6/10\n",
      "86/86 [==============================] - 31s - loss: 0.6781 - acc: 0.5465    \n",
      "Epoch 7/10\n",
      "86/86 [==============================] - 30s - loss: 0.6465 - acc: 0.6047    \n",
      "Epoch 8/10\n",
      "86/86 [==============================] - 29s - loss: 0.6210 - acc: 0.7442    \n",
      "Epoch 9/10\n",
      "86/86 [==============================] - 30s - loss: 0.5810 - acc: 0.7209    \n",
      "Epoch 10/10\n",
      "86/86 [==============================] - 32s - loss: 0.5137 - acc: 0.8140    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb58e602e8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_pad_char, y_train_num, epochs=10, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 3s\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test_pad_char, y_test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusions\n",
    "\n",
    ":("
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvltp",
   "language": "python",
   "name": "venvltp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
